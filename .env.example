# =============================================================================
# Database Configuration
# =============================================================================
DATABASE_URL=postgresql://user:password@localhost:5432/podcast_db

# =============================================================================
# Ollama Configuration
# =============================================================================
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBEDDING_URL=http://localhost:11435
OLLAMA_MODEL=qwen3:4b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# =============================================================================
# Reranker Service
# =============================================================================
ENABLE_RERANKER=true
RERANKER_URL=http://localhost:8080
RERANKER_TIMEOUT=5000

# =============================================================================
# Quote Processing
# =============================================================================
# Enable quote finding and entailment validation
# Set to false to skip quote processing and only extract claims (much faster)
# Trade-off: No quote evidence or validation, but ~85-90% faster extraction
ENABLE_QUOTE_PROCESSING=true

# =============================================================================
# Text Processing
# =============================================================================
CHUNK_SIZE=16000
CHUNK_OVERLAP=1000

# Parallel processing configuration (DSPy asyncify concurrency limits)
MAX_CLAIM_EXTRACTION_CONCURRENCY=3  # Max concurrent claim extraction calls
MAX_ENTAILMENT_CONCURRENCY=10  # Max concurrent entailment validation calls
MAX_AD_CLASSIFICATION_CONCURRENCY=10  # Max concurrent ad classification calls
AD_CLASSIFICATION_BATCH_SIZE=10  # Claims per LLM call (true batching, not parallelism)

# =============================================================================
# Deduplication Thresholds
# =============================================================================
# Enable cross-episode deduplication (checks database for duplicates)
# Set to false to skip database dedup and treat all claims as unique
ENABLE_CROSS_EPISODE_DEDUPLICATION=true

EMBEDDING_SIMILARITY_THRESHOLD=0.85
RERANKER_VERIFICATION_THRESHOLD=0.9
STRING_SIMILARITY_THRESHOLD=0.95
VECTOR_DISTANCE_THRESHOLD=0.15

# =============================================================================
# Scoring Configuration
# =============================================================================
MIN_CONFIDENCE=0.3
MAX_QUOTES_PER_CLAIM=10
MIN_QUOTE_RELEVANCE=0.85

# =============================================================================
# Caching
# =============================================================================
CACHE_MAX_SIZE=10000
CACHE_TTL_HOURS=1

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=INFO

# =============================================================================
# Claim Quality Filtering
# =============================================================================
# Enable specificity filter (claims must have numbers, proper nouns, or dates)
# Set to false to keep all extracted claims regardless of specificity
ENABLE_CLAIM_SPECIFICITY_FILTER=false

# Maximum claims to extract per chunk (prevents oversized batches from dense content)
# Keeps longest/most detailed claims when limit is exceeded
MAX_CLAIMS_PER_CHUNK=10
FILTER_LOWERCASE_CLAIMS=true

# =============================================================================
# Ad Classification
# =============================================================================
FILTER_ADVERTISEMENT_CLAIMS=true

# Confidence threshold for classifying claims as advertisements (0.0-1.0)
# Higher = more conservative (fewer false positives)
# Lower = more aggressive (fewer false negatives)
# Recommended: 0.7 (balanced), 0.8-0.9 (conservative), 0.5-0.6 (aggressive)
AD_CLASSIFICATION_THRESHOLD=0.7

# =============================================================================
# DSPy Model Paths
# =============================================================================
CLAIM_EXTRACTOR_MODEL_PATH=models/claim_extractor_llm_judge_v1.json
ENTAILMENT_VALIDATOR_MODEL_PATH=models/entailment_validator_v1.json
AD_CLASSIFIER_MODEL_PATH=models/ad_classifier_v1.json

# =============================================================================
# MIPROv2 Optimization (Optional)
# =============================================================================
# Use Claude for instruction proposal (Qwen3 4B struggles with meta-prompting)
# Options: 'claude-haiku-4-5' (cheap/fast) or 'claude-sonnet-4-5-20250929' (better)
MIPRO_PROMPT_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_API_KEY=

# =============================================================================
# Gemini Claim Validation
# =============================================================================
# Google Gemini API key for claim validation
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Gemini model for validation (gemini-2.0-flash-exp recommended for speed)
# Options: gemini-2.0-flash-exp (fast), gemini-2.0-flash (stable), gemini-1.5-pro (accurate)
GEMINI_MODEL=gemini-2.0-flash-exp

# Number of claims to validate per Gemini API call (batch processing)
# Higher = faster but more tokens per call. Recommended: 50
GEMINI_VALIDATION_BATCH_SIZE=50

# Gemini API request timeout in seconds
GEMINI_TIMEOUT=30

# Minimum number of unverified claims an episode must have to be eligible for validation
# Episodes with fewer unverified claims are skipped (guardrail)
MIN_CLAIMS_FOR_VALIDATION=5

# =============================================================================
# Premium Claim Extraction (Gemini 3)
# =============================================================================
# Model for premium claim extraction with 1M context window
# Use gemini-3-pro-preview for best results
GEMINI_PREMIUM_MODEL=gemini-3-pro-preview

# Temperature for premium extraction (0 = deterministic)
GEMINI_PREMIUM_TEMPERATURE=0.0

# =============================================================================
# API Configuration
# =============================================================================
# API timeout in seconds (0 = no timeout, recommended for long-running batch operations)
API_TIMEOUT=0
API_HOST=0.0.0.0
API_PORT=8000
API_KEY=

# Other Settings
ENABLE_QUOTE_PROCESSING=false
