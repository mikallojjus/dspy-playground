# =============================================================================
# Database Configuration
# =============================================================================
DATABASE_URL=postgresql://postgres:testpass@localhost:5433/test_db

# =============================================================================
# Ollama Configuration - Dual Instance Setup
# =============================================================================
# Separate instances eliminate pinned memory conflicts between models
OLLAMA_URL=http://localhost:11434                # LLM instance (Qwen 2.5 7B)
OLLAMA_EMBEDDING_URL=http://localhost:11435      # Embedding instance (nomic-embed-text)
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_0
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# Context window size: 8192=~5GB VRAM, 16384=~6GB VRAM, 32768=~7.6GB VRAM per instance
OLLAMA_NUM_CTX=16384

# =============================================================================
# Reranker Service
# =============================================================================
ENABLE_RERANKER=true
RERANKER_URL=http://localhost:8080
RERANKER_TIMEOUT=5000

# =============================================================================
# Text Processing
# =============================================================================
CHUNK_SIZE=16000
CHUNK_OVERLAP=1000
PARALLEL_BATCH_SIZE=3

# =============================================================================
# Deduplication Thresholds
# =============================================================================
EMBEDDING_SIMILARITY_THRESHOLD=0.85
RERANKER_VERIFICATION_THRESHOLD=0.9
STRING_SIMILARITY_THRESHOLD=0.95
VECTOR_DISTANCE_THRESHOLD=0.15

# =============================================================================
# Scoring Configuration
# =============================================================================
MIN_CONFIDENCE=0.3
MAX_QUOTES_PER_CLAIM=10
MIN_QUOTE_RELEVANCE=0.85

# =============================================================================
# Quote Optimization (Coarse Chunking + DSPy)
# =============================================================================
# Coarse chunk size in tokens (reduces embeddings by 50x vs fine-grained segments)
COARSE_CHUNK_SIZE=300
# Overlap between coarse chunks in tokens
COARSE_CHUNK_OVERLAP=50
# Number of top relevant chunks to retrieve per claim
TOP_K_CHUNKS=4
# Minimum confidence threshold for quote verification (0.0-1.0)
# 0.90 = strict, 0.85 = moderate, 0.80 = lenient, 0.70 = very lenient
QUOTE_VERIFICATION_MIN_CONFIDENCE=0.70
# Path to optimized DSPy QuoteFinder model (leave empty to use zero-shot baseline)
QUOTE_FINDER_MODEL_PATH=
# Enable entailment validation
ENABLE_ENTAILMENT_VALIDATION=false

# =============================================================================
# DSPy Optimization (applies to all training: claims, entailment, quotes)
# =============================================================================
# Number of bootstrapped few-shot examples in prompts (more = better quality, slower inference)
DSPY_MAX_BOOTSTRAPPED_DEMOS=4
# Number of seed examples in prompts (internally set to match BOOTSTRAPPED_DEMOS)
DSPY_MAX_LABELED_DEMOS=2
# Optimization rounds (more = better quality, longer training time)
DSPY_MAX_ROUNDS=3
# Maximum errors tolerated during optimization before failing
DSPY_MAX_ERRORS=5

# =============================================================================
# Caching
# =============================================================================
CACHE_MAX_SIZE=10000
CACHE_TTL_HOURS=1

# =============================================================================
# Logging
# =============================================================================
LOG_LEVEL=debug
